\chapter{Improvements}\label{ch:improvements}
In this section we will describe the various changes
that we applied to the default kernel search implementation,
with the objective of improving its performance
in solving the MKP instances.

For the sake of completeness, we well also describe the all attemps at improving
the algorithm, thus including the unsuccesful ones.

The effectiveness of the tuning is demonstrated using the selected instances
described in~\ref{subsec:inst} as benchmark.


%\section{Variable Sorting}


\section{Kernel Construction Criteria}
A recurring problem we have identified when running more complex
instances (such as the one in \texttt{FK\_4}) is that,
from the very beginning, the kernel contains quite a lot of variables,
which makes the solving of the kernel problem and the buckets sub-problems slow.

This means that a starting point to improve the performance of the Kernel Search
could be to optimize the management of the kernel set,
possibily starting from the \textit{kernel construction criterion},
which by default is to include the first \textit{C} sorted variables.

We have experimented with the following criteria:
\begin{enumerate}
    \item Select the variables that have a greater than 0 in the LP relaxation;
    \item Select the variables that have a value equal to 1 in the LP relaxation;
    \item Select the variables that have a value greater than a threshold in the LP relaxation.
\end{enumerate}

However, after running benchmark to compare these criteria
with the default one, the results demonstrated that the default criterion
consistently gave better results.
It is however worth noting that the criterion 2 and 3 (with a proper threshold),
gave results that were quite close to the ones with the default criteria,
while using much less time.


\section{Ejection of Variables from the Kernel}
As an alternative to improve the efficiency in solving the kernel set
we have developed a \textit{kernel eject} method,
which consists in removing from the kernel variables
that do not appear in the solution of \textit{n}
consecutive bucket sub-problems.

When the variables are removed from the kernel
not deleted from the model, but they are returned to their original bucket.
If they originally were in the kernel, they are inserted into the first bucket.


\section{Repetition Counter}
An observation we could make is that when solving buckets,
after a certain number of iterations the algorithm
repeatedly finds new solutions with the same objective value,
and sometimes it even struggles to find new solutions at all.
In other words, the algorithm gets stuck in \textit{local optima},
from which it's hard to escape.

Our hypothesis is that there are two causes for this:
\begin{enumerate}
    \item The GUROBI solver cannot find a (better) solution for the sub-problem,
    either due to the infeasibility of the problem or because of
    the time limit set for solving each bucket.
    \item The kernel search algorithm, during its improvement phase,
    only accepts a new solution (thus updating the kernel)
    if it improves upon, or is at least equal, to the incumbent one.
    This is done by adding a cutoff constraint, as explained in~\ref{sec:improving-efficiency}.
\end{enumerate}

In order to mitigate this problem we introduced a \textit{repetition counter}
that, during the improvement phase of kernel search,
removes the cutoff constraint for \textit{k} buckets
when the same solution (or no solution) is found for \textit{h} times.

The idea is that this method allows to select if the focus should be on
\textit{diversification} or \textit{intensification},
by appropriately setting parameters \textit{h} and \textit{k}.

A low value for \textit{h} and a high one for \textit{k} allow for
diversification, by adding to the kernel variables that otherwise
may never be selected, and could allow to escape the local optimum.

On the opposite, a high value for \textit{h} and a low one for \textit{k}
switch to focus on intensification, by giving priority to finding
better solutions.

After experimenting with different values for \textit{h} and \textit{k},
we found that keeping them more or less equal allowed for
a reasonable balance between diversification and intensification.
In particular we found that \(h=3\) and \(k=3\) worked quite well
with the test instances we selected.

\subsection{Reset counter when new optimum is found}
A small enhancement we have applied to this mechanism
is to reset the counter to its initial status
whenever the Kernel Search finds a new solution
that is better than any other found before.

This is significant because the default repetition counter
method completely ignores the value of the solutions found
during the \textit{k} cycles.


\section{Item Dominance}
In~\cite{mkp:2019} an improvement for the MKP model is suggested:
given two items \textit{j} and \textit{k}, if \(w_{j} \leq w_{k}\)
and \(p_{j} \geq p_{k}\), then it is said that \textit{j dominates k}.
This means that when an item is excluded from the solution,
all items dominated by it can also be excluded.

The simplest way to implement this method is to preliminarly sort all
items according to non-increasing weight, breaking ties by
non-decreasing profit.
For each item \(k\), items \(j \coloneqq k+1,\dots,n\) are parsed,
and if \(p_{j} \geq p_{k}\) then the pair \((j,k)\) is added to a
dominance list \(D\).
Then, the following constraints are added to the model:
\begin{equation}
    \label{eq:itemdom}
    \sum_{i=1}^{m} x_{ij} \geq \sum_{i=1}^{m} x_{ik} \qquad (j,k) \in D
\end{equation}

%\section{Iterative Kernel Search}
